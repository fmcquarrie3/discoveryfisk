{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Import packages\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get function to get raw HTML\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #closing ensures any network resources are freed when out of scope - good practice\n",
    "        with closing(get(url, stream=True)) as resp: \n",
    "            # I added an extra if statement to check for redirects\n",
    "            if is_redirect(resp):\n",
    "                return \"redirect\"\n",
    "            elif is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "# I added this helper function to check if the HTML response returned by the browser had been redirected at some point\n",
    "# See http://docs.python-requests.org/en/v0.10.6/api/ for docs on history attribute\n",
    "def is_redirect(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the resp had been redirected at some point in the retrieval process due to nonexistent url, \n",
    "    False otherwise. \n",
    "    Arguments: a HTML response object\n",
    "    \"\"\"\n",
    "    resp_history = resp.history\n",
    "    if resp_history: #not empty - some things happend before response was returned\n",
    "        # I specifically want to check for redirects (status code 302)\n",
    "        statuses = [h.status_code==302 for h in resp_history]\n",
    "        # Are there any true in the above list comp? Then something was redirected. \n",
    "        return np.any(statuses)\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(parsed_html):\n",
    "    \"\"\"Parses the author name from a Medium article. \n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Returns: \n",
    "        the author name as a string or None if author tag not present\n",
    "    \"\"\"\n",
    "    author = parsed_html.find('meta', property=\"author\")\n",
    "    return author['content'] if author else None\n",
    "\n",
    "def get_author_bio(parsed_html):\n",
    "    \"\"\"Parses the author's bio/description from a Medium article. \n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Returns: \n",
    "        the author bio/description as a string if it exists, and None otherwise\n",
    "    \"\"\"\n",
    "    bios = parsed_html.find_all('div', class_=\"ui-caption ui-xs-clamp2 postMetaInline\")\n",
    "    # If bios is empty, that means there is no author bio for article, and the [0]  will error so we need to explicityly\n",
    "    # check and return None if no author bio\n",
    "    return bios[0].text if bios else None\n",
    "\n",
    "def get_title(parsed_html):\n",
    "    \"\"\"Parses the title of a Medium article.\n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Returns: \n",
    "        the title of the article as a string or None if title tag not present\n",
    "    \"\"\"\n",
    "    title = parsed_html.find('meta', property='og:title')\n",
    "    return title['content'] if title else None\n",
    "\n",
    "def get_raw_publish_date(parsed_html):\n",
    "    \"\"\"Parses the date a Medium article was published. \n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Returns: \n",
    "        a raw/uncleaned publish date, which looks like '2016-11-19T16:48:30.365Z', or None if tag not present\n",
    "    \"\"\"\n",
    "    date = parsed_html.find('meta', property='article:published_time')\n",
    "    return date['content'] if date else None\n",
    "\n",
    "def get_article_publisher(parsed_html):\n",
    "    \"\"\"Parses the article's publisher from a Medium article. \n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Notes:\n",
    "        The publisher is encoded as \"https://facebook.com/publisher\" so I extract just the publisher name.\n",
    "        Not all articles have a verified publisher - like if it's just the author's personal blog - so publisher is\n",
    "        just \"medium\" in that case\n",
    "    Returns:\n",
    "        If article is hosted on verified publisher, returns publisher name as string\n",
    "        If article is on personal blog, returns \"medium\" as a string. \n",
    "        If publisher tag doesn't exist, returns None\n",
    "        \n",
    "    \"\"\"\n",
    "    long_publisher = parsed_html.find(\"meta\", property='article:publisher')\n",
    "    return long_publisher['content'].split(\"/\")[3] if long_publisher else None\n",
    "\n",
    "def get_raw_article_text(parsed_html):\n",
    "    \"\"\"Extracts out the text/content of the Medium article.\n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Returns:\n",
    "        a raw/uncleaned string of text or None if tag is not present.\n",
    "        The string is considered \"raw\" because there are some weird characters that are remants\n",
    "        of header formatting and the like.\n",
    "    \"\"\"\n",
    "    text = parsed_html.find_all('div', class_='postArticle-content')\n",
    "    return text[0].text if text else None\n",
    "\n",
    "def get_all_article_info(article_url):\n",
    "    \"\"\"Parses a Medium article to get all needed information about author and story.\n",
    "    Arguments:\n",
    "        article_url: String of url for article to parse\n",
    "    Returns:\n",
    "        list of [author, author_bio, title, date, publisher article_text], where each component is a string or None\n",
    "    \"\"\"\n",
    "    raw_html = simple_get(article_url)\n",
    "    parsed_html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    author = get_author(parsed_html)\n",
    "    author_bio = get_author_bio(parsed_html)\n",
    "    title = get_title(parsed_html)\n",
    "    date = get_raw_publish_date(parsed_html)\n",
    "    publisher = get_article_publisher(parsed_html)\n",
    "    text = get_raw_article_text(parsed_html)\n",
    "    return [author, author_bio, title, date,publisher, text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_open_post_links(parsed_html):\n",
    "    \"\"\"Retrieves all links on a page that open to an article (does not return \"home\", \"sign up\" links etc).\n",
    "    Arguments:\n",
    "        parsed_html: object returned by calling `BeautifulSoup(raw_html, 'html.parser')`\n",
    "    Notes:\n",
    "        All links on a page that have the data-attribute 'open-post' are the types of links we want.\n",
    "        find_all returns a special BeautifulSoup object so I need to extract the string url.\n",
    "        I think there are muliple links for each post (like the title and \"read more\"), so returning array \n",
    "        has duplicates\n",
    "    Returns:\n",
    "        List of link strings\n",
    "    \"\"\"\n",
    "    def href_open_post_data_action(tag):\n",
    "        \"\"\"Helper parsing function that checks to see if an \"a\" tag is an href with an 'open-post' data action.\n",
    "        Arguments:\n",
    "            tag: All html tags like <a>, <div> that BeautifulSoup has extracted\n",
    "        Returns:\n",
    "            true if tag has href and 'open-post' data-action attribute, false otherwise\n",
    "        \"\"\"\n",
    "        return tag.has_attr('href') and tag.get('data-action') == 'open-post'\n",
    "    \n",
    "    link_objects =  parsed_html.find_all(href_open_post_data_action)\n",
    "    return [link.get('href') for link in link_objects] #retrieves the string url from link object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates_for_url(integer_date):\n",
    "    \"\"\"\n",
    "    Formats dates to enter into the archive url, which requires numbers < 10 to be encoded with a \"0\" in front,\n",
    "    but Python doesn't allow 01 as an integer. \n",
    "    \"\"\"\n",
    "    if integer_date < 10:\n",
    "        return str(0) + str(integer_date)\n",
    "    else:\n",
    "        return str(integer_date)\n",
    "def get_specific_Codingbootcamp_links():\n",
    "    # For current range of years available\n",
    "    years = range(2013, 2019)\n",
    "    months_w_o_zero_in_front = range(1, 13)\n",
    "    days_w_o_zero = range(1, 32)\n",
    "    base_url = \"https://medium.com/tag/codingbootcamp/archive\"\n",
    "    # Collector variable to story all story urls\n",
    "    story_links = []\n",
    "    for year in years:\n",
    "        # These years don't have month, day subdivisions so to prevent redundant parsing, \n",
    "        # just parse the base year and its links.\n",
    "        if year == 2013 or year == 2014:\n",
    "            # \n",
    "            url = base_url + \"/\" + format_dates_for_url(year)\n",
    "            raw_year_html = simple_get(url)\n",
    "            parsed_year_html = BeautifulSoup(raw_year_html, 'html.parser')\n",
    "            links = get_all_open_post_links(parsed_year_html)\n",
    "            #use extend instead of append because just want to add elements, not create nested lists.\n",
    "            story_links.extend(links) \n",
    "        else:\n",
    "            for month in months_w_o_zero_in_front:\n",
    "                for day in days_w_o_zero:\n",
    "                    url = base_url + \"/\" + format_dates_for_url(year) + format_dates_for_url(month) + \"/\" + format_dates_for_url(day)\n",
    "                    raw_day_html = simple_get(url)\n",
    "                    # 2015, 2016, 2017, 2018 have some dates with no stories, so GET requests are redirected\n",
    "                    # and we don't want to parse stuff we already did.\n",
    "                    if raw_day_html == \"redirect\":\n",
    "                        #skip the redirected day - advance in for loop\n",
    "                        break\n",
    "                    parsed_day_html = BeautifulSoup(raw_day_html, 'html.parser')\n",
    "                    links = get_all_open_post_links(parsed_day_html)\n",
    "                    #use extend instead of append because just want to add elements, not create nested lists.\n",
    "                    story_links.extend(links)\n",
    "    # Create a set of links to remove duplicates and then turn back into a list (better data structure)\n",
    "    return list(set(story_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Redirect Hack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to move this to a different notebook because my intial exploration was getting pretty cluttered.  \n",
    "The issue is:\n",
    "* If you attempt to access a date in the tag archive that doesn't exist, you are redirected back to the main year or month page. I don't want to reparse something I've already parsed, so I modified the `simple_get` function that if you are redirected when attempting to access a url, the function doesn't return the html and its not parsed. \n",
    "* However we just discovered that some of the article urls (like ones that direct to a single article on medium not through the archive or anything) had redirects in their history, which meant that `simple_get` failed to return the html and we didn't parse any necessary information about that article. \n",
    "\n",
    "From these conclusions, the exact code in simple get right now can't work. So we have to figure out another \"marker/cue\" that would indicate something has been redirected, but only redirected back to a different url.   \n",
    "To experiement, `good_url` is an article url that has redirects in its history, but is an article we want to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_url = \"https://medium.freecodecamp.org/5-key-learnings-from-the-post-bootcamp-job-search-9a07468d2331?source=search_post---------0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to define a modified version of simple_get that I can play around with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_resp(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #closing ensures any network resources are freed when out of scope - good practice\n",
    "        with closing(get(url, stream=True)) as resp: \n",
    "            # I added an extra if statement to check for redirects\n",
    "            return resp\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [302]>, <Response [302]>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_resp = get_html_resp(good_url)\n",
    "good_resp.history #302 is redirect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weird thing is, I don't understand why it had redirects because I don't see the url change when I type it into the search bar - like when I attempt to access a nonexistent archive url, I am redirected to a different url.  So that is my idea on how to fix this bug, is to check to see if the url I passed in is different than the one I get back.  \n",
    "I am going to define a variable for a non-existent achrive url so I can see if I can find any differences that I can exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_archive_url = \"https://medium.com/tag/codingbootcamp/archive/2013/01/02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [302]>, <Response [302]>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_resp = get_html_resp(bad_archive_url)\n",
    "bad_resp.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.freecodecamp.org/5-key-learnings-from-the-post-bootcamp-job-search-9a07468d2331?source=search_post---------0&gi=d175e8d74f15'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_resp.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.com/tag/codingbootcamp/archive/2013'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_resp.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah ah! It looks like for a bad response, one that was redirected and we don't want to parse, will have a different url in the final response than the one that was passed in. Let's see if we can write a function to check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_is_different(resp, original_url):\n",
    "    return resp.url != original_url\n",
    "def get_response_url_check(url):\n",
    "    try:\n",
    "        #closing ensures any network resources are freed when out of scope - good practice\n",
    "        with closing(get(url, stream=True)) as resp: \n",
    "            # I added an extra if statement to check for redirects\n",
    "            if url_is_different(resp, url):\n",
    "                return \"redirect\"\n",
    "            elif is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'redirect'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_url_check(good_url) #should be response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oh I can see in a difference checker that the numbers at the end of the urls are different... hmm there goes my big idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.freecodecamp.org/5-key-learnings-from-the-post-bootcamp-job-search-9a07468d2331?source=search_post---------0&gi=d175e8d74f15'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_resp.request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.freecodecamp.org/5-key-learnings-from-the-post-bootcamp-job-search-9a07468d2331?source=search_post---------0&gi=d175e8d74f15'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_resp.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.com/tag/codingbootcamp/archive/2013'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_resp.request.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.com/tag/codingbootcamp/archive/2013'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_resp.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok I am not having much luck - what I might do is have simple_get return raw html and the url of the resp object, and check to see if the the url of the response is not the same as what I passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
